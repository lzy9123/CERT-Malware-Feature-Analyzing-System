# Feature Analyzing System's Developer's Guide
This document describes how developers can extend the feature analysis system, including adding new models, adding new component to the pipeline and adding new data format support.


## Overview
### Environment
- python3
- Linux version 4.4. MacOs 10.14
- Pandas v0.24.0
- Numpy v1.16
- Sklearn v0.21.2

### Download
```bash
git clone https://github.com/lcyyzy/MITS-CERT-Project.git
```

## Architecture
https://docs.google.com/document/d/1DIFo1QT1YnJpS9Yp27Iqi2YIzMwlVrP2XcbU78tqtx8/edit#heading=h.nj23sjpj5u97

## Pipeline
The system supports processing data step by step.

```__init__```: initialize the pipeline class.

```run```/ ```__call__```: run the pipeline step by step. ```__call__``` is special method triggered when the instance of a class is called.

```fit```: train the model in the pipeline.

```_validate_steps```: check the step when initializing the pipeline.


Steps includes Models, function (method), plot or adding data: model step will output predicteded result, function (method) step will output the data after calling the function, plot step will draw the picture. And if another piece of data is parsed in the step, it will itself to previous output as a new tuple.

Developers can extend the pipline by define and adding new step types in the pipeline.

## Data Reader

This module is used to read all formats of input data, transfer them into other format or read data from other DataReader and transfer them into different types of data format.

DataReader should extend class MLPipeLine.DataReader and implement 6 methods to make it works: readFromPath(path) Read data from file path. readFromData(data) Read data from other DataReader.getData() getBatch(batch_size) Return a new DataReader containing only batch_size of data resetBatch() Reset the iterater of data iter() / next() DataReader should support iterator to iterate data. getData() Return data for next DataReader

### Data Reader Module (read_data.py)

Data Reader provides several functions to support the system to read and preprocess the input data. 

```read_file()```: Call specific read file functions for different file format.

```find_features()```: Find all features' name in the data.

```format_feature()```: Transfer the data into a matrix.

```save_to_csv()```: Save the matrix into a csv file for future use.

Developers can extend the data reader to support to read different format of data.

### Add new file readers
Add a new file reader function into the data reader module (read_data.py).
```Python3
import json

def read_json_file(file_path):
    data = []
    with open(file_path) as f:
        for line in f:
            data.append(json.loads(line))
    return data
```

### Save in different file formats
Add a new save file function into the data reader module (read_data.py).
```Python3
import pandas as pd

def save_to_csv(feature, feature_names):
    save = pd.DataFrame(columns=feature_names, data=feature)
    save.to_csv('./formatted_feature_matrix.csv', encoding = 'utf-8')
```


## Models
### Model Class (Model.py)

The system supports multiple machine learning models to do different data analyzing works. All models need to extend the abstract class Model (Model.py) in order to fit into the Pipeline.

- Model Class
<img src="https://github.com/lcyyzy/MITS-CERT-Project/blob/master/User%20Guide%20Pictures/modelclass.png" title="Model class" width="300" height="150" />

Developers can extend the models by using the thrid party machine learning libraries such as  [sklearn](https://scikit-learn.org/stable/).

```load```: load data from the pre-trained model.

```save```: save the trained model to the local directory.

```_fit```: internal function for the training process of the model.

```fit```: abstract fit function to be overridden by specific models.

```predict```:  predict the result given the input testing dataset.

```__class__```:  get the class type of models (used for pipeline step identification).


### Add new models
This part will give an example of adding new models step by step.
1. Determining the input and output data format of the model. If the inpur format is not supported by the current data reader, the developer needs to extend the new data reader in order to dock input data.

2. Finding functions from the third party machine learning libraries in terms of the user's need. Or, the developers can also choose to write their own code to build the model.

3. Extending the Model.py to create a new model class
```python3
from .Model import Model

class Kmeans(Models):
    def fit(self, x_train, y_train = None, **parameter):
        self.modelType = KMeans(**parameter)
        self._fit(x_train, None)  
```

4. Using the the instance of the new model to build the pipeline
```python3
from Model import Similarity
from utils.Pipeline import PipeLine

step = [
  preprocess1,
  preprocess2,
  Similarity(metric)
]

Pipeline1=Pipeline(step)
```

5. Use the pipeline to train the model
```python3
step = [
        preprocess1,
        preprocess2,
        MLP('Neural Networks')
]

pipeline1.fit(train_data[['size', 'vt_meta_size']], train_data['isShared-lib'], 
              solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 2), random_state=1)
```
